<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models | </title> <meta name="author" content="Spectrum Lab"> <meta name="description" content="A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias."> <meta name="keywords" content="IISc, Bengaluru, Indian Institute of Science, signal processing, machine learing, computational imaging, reserach, generative modelling, artificial intelligence"> <meta property="og:site_name" content=""> <meta property="og:type" content="article"> <meta property="og:title" content=" | Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models"> <meta property="og:url" content="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2025/distillation-and-fairness/"> <meta property="og:description" content="A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models"> <meta name="twitter:description" content="A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias."> <meta name="twitter:site" content="@SpectrumLabIISc"> <meta name="twitter:creator" content="@SpectrumLabIISc"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Spectrum Lab"
        },
        "url": "https://spectrum-lab-iisc.github.io/spectrum-website/blog/2025/distillation-and-fairness/",
        "@type": "BlogPosting",
        "description": "A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias.",
        "headline": "Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models",
        
        "sameAs": ["https://scholar.google.com/citations?user=1g1i1B4AAAAJ", "https://github.com/spectrum-lab-iisc", "https://twitter.com/SpectrumLabIISc"],
        
        "name": "Spectrum Lab",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/spectrum-website/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/spectrum-website/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://use.typekit.net/pzd6qvr.css"> <link defer rel="stylesheet" href="/spectrum-website/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/spectrum-website/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2025/distillation-and-fairness/"> <script src="/spectrum-website/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/spectrum-website/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/spectrum-website/assets/js/distillpub/template.v2.js"></script> <script src="/spectrum-website/assets/js/distillpub/transforms.v2.js"></script> <script src="/spectrum-website/assets/js/distillpub/overrides.js"></script> <style type="text/css">.MJX-TEX,.MJX-TEX *{font-family:"Times New Roman"}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models",
            "description": "A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias.",
            "published": "March 31, 2025",
            "updatedDate": "May 16, 2025",
            "postAuthor": "Yani Ioannou",
            "authors": [
              
              {
                "author": "Aida Mohammadshahi",
                "authorURL": "http://github.com/aidamohammadshahi",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yani Ioannou",
                "authorURL": "https://yani.ai",
                "affiliations": [
                  {
                    "name": "University of Calgary",
                    "url": ""
                  }
                ]
              }
              
            ],
            "doi": "10.48550/arXiv.2410.08407",
            "url": "https://openreview.net/pdf?id=xBbj46Y2fN",
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/spectrum-website/"><img class="navbar spectrum-logo-light" src="/spectrum-website/assets/img/SpectrumLab/SpectrumLogo%20Black.png" alt=""> <img class="navbar spectrum-logo-dark" src="/spectrum-website/assets/img/SpectrumLab/SpectrumLogo%20White.png" alt=""> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/news/">news </a> </li> <li class="nav-item active"> <a class="nav-link" href="/spectrum-website/blog/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/opportunities/">opportunities </a> </li> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/alumni/">alumni </a> </li> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/recognition/">recognition </a> </li> <li class="nav-item "> <a class="nav-link" href="/spectrum-website/contactus/">contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models</h1> <p>A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#tldr">TL;DR</a></li> <li class="toc-entry toc-h2"><a href="#introduction-knowledge-distillation">Introduction: Knowledge Distillation</a></li> <li class="toc-entry toc-h2"> <a href="#understanding-knowledge-distillation">Understanding Knowledge Distillation:</a> <ul> <li class="toc-entry toc-h3"><a href="#neural-networks-as-function-approximators">Neural Networks as Function Approximators</a></li> <li class="toc-entry toc-h3"><a href="#the-concept-of-dark-knowledge">The Concept of “Dark Knowledge”</a></li> <li class="toc-entry toc-h3"><a href="#the-role-of-temperature-in-softmax">The Role of Temperature in Softmax</a></li> <li class="toc-entry toc-h3"><a href="#the-distillation-process">The Distillation Process</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#beyond-accuracy-does-the-student-learn-the-same-function">Beyond Accuracy: Does the Student Learn the Same Function?</a></li> <li class="toc-entry toc-h2"> <a href="#research-deep-dive-unpacking-the-impact-of-distillation">Research Deep Dive: Unpacking the Impact of Distillation</a> <ul> <li class="toc-entry toc-h3"><a href="#research-questions">Research Questions</a></li> <li class="toc-entry toc-h3"><a href="#analyzing-class-wise-bias">Analyzing Class-wise Bias</a></li> <li class="toc-entry toc-h3"><a href="#probing-group-fairness">Probing Group Fairness</a></li> <li class="toc-entry toc-h3"><a href="#investigating-individual-fairness">Investigating Individual Fairness</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#key-findings-and-insights">Key Findings and Insights</a> <ul> <li class="toc-entry toc-h3"><a href="#class-wise-bias-an-uneven-impact">Class-wise Bias: An Uneven Impact</a></li> <li class="toc-entry toc-h3"> <a href="#group-fairness-temperature-matters">Group Fairness: Temperature Matters</a> <ul> <li class="toc-entry toc-h4"><a href="#very-high-temperatures">Very High Temperatures</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#individual-fairness-consistency-improves">Individual Fairness: Consistency Improves</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#conclusion-distillation-a-double-edged-sword">Conclusion: Distillation, A Double-Edged Sword?</a></li> <li class="toc-entry toc-h2"><a href="#future-directions">Future Directions</a></li> <li class="toc-entry toc-h2"><a href="#citing-our-work">Citing our work</a></li> </ul> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <p>Knowledge Distillation (or distillation) is a technique used to compress large AI models into smaller, more efficient versions. For example, DeepSeek R1 with 671 billion parameters <d-cite key="DeepSeek2024v3"></d-cite>, was distilled into smaller, more manageable versions that are easier to deploy in real-world applications.</p> <p>While distillation often succeeds in maintaining overall accuracy, our recently accepted Transactions in Machine Learning Research (TMLR) paper, “<a href="https://openreview.net/pdf?id=xBbj46Y2fN" rel="external nofollow noopener" target="_blank">What’s Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias</a>” <d-cite key="Mohammadshahi2025distillation"></d-cite> explores how the distillation process affects model decisions, particularly in terms of fairness and bias. We found that:</p> <ul> <li>The distillation temperature significantly influences the biases of the student model relative to the teacher model, and a smaller student model trained from scratch.</li> <li>Higher distillation temperatures generally lead to distilled models that make more fair decisions, i.e. improved group fairness and individual fairness metrics.</li> <li>Surprisingly, distilled models trained at high temperatures rarely used in practice, e.g. $T=10$, can be fairer than their larger teacher counterparts.</li> <li>This research highlights the need to consider fairness implications when using distillation, especially in sensitive applications where impactful decisions are made, like hiring or loan approvals.</li> </ul> <h2 id="introduction-knowledge-distillation">Introduction: Knowledge Distillation</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_teachertostudent_simple.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_teachertostudent_simple.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Distillation of a smaller student from a larger teacher model." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Large models, like DeepSeek R1 with 671 billion parameters <d-cite key="DeepSeek2024v3"></d-cite>, are often distilled into smaller, more manageable versions (e.g., 1.5-70B Llama models) that are easier to deploy in real-world applications. This process, known as Knowledge Distillation (or just distillation) <d-cite key="Hinton2015distilling"></d-cite>, aims to transfer the “knowledge” from a large “teacher” model to a smaller “student” model, often preserving overall performance like test accuracy.</p> <p>While distillation often succeeds in maintaining overall accuracy, our recenly accepted Transactions in Machine Learning Researc (TMLR) paper, “<a href="https://openreview.net/forum?id=xBbj46Y2fN" rel="external nofollow noopener" target="_blank">What’s Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias</a>” <d-cite key="Mohammadshahi2025distillation"></d-cite>, takes a deeper dive into understanding how distillation affects the decisions made by a model, through the lens of fairness and bias. This is particularly important as AI systems are increasingly used in sensitive areas like hiring, loan applications, and medical diagnosis, where fairness is crucial.</p> <p><strong>Does the distilled student model treat all groups and types of data the same way the teacher did, or does the process introduce new, potentially harmful, biases?</strong> To grasp the implications of KD, let’s first revisit some core concepts.</p> <h2 id="understanding-knowledge-distillation">Understanding Knowledge Distillation:</h2> <h3 id="neural-networks-as-function-approximators">Neural Networks as Function Approximators</h3> <p>At their heart, neural networks are powerful function approximators. They learn a function $f$ that maps an input $\mathbf{x}$ to an output $y$ (or a probability distribution $p$ over possible outputs in classification tasks),</p> \[f(\mathbf{x}) = y ,\] <p>where $f$ is the model, $\mathbf{x}$ is the input (like an image or text), and $y$ is the output (like a label or a probability distribution). The goal of training is to minimize the difference between the model’s predictions and the true labels, often using a loss function like cross-entropy.</p> <h3 id="the-concept-of-dark-knowledge">The Concept of “Dark Knowledge”</h3> <div class="row align-items-center"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_softtargets_catdogairplane.png" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_softtargets_catdogairplane.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row align-item-center justify-content-center"> \[\require{colorv2} \Large f(\textcolor{red}{\mathbf{x} \textrm{: image of cat}}) = \{ \textcolor{green}{\textrm{dog: } 0.09}, \textcolor{red}{\textrm{cat: } 0.9}, \textcolor{blue}{\textrm{airplane: } 0.01}\}\] </div> <p>Trained models, especially large ones, learn much more than just how to map inputs to correct labels. They capture a rich, nuanced understanding of the data’s structure and relationships. For example, an ImageNet model doesn’t just learn to identify a “cat”; it also implicitly learns that a cat is more similar to a “dog” than to an “airplane”. In this example, the model is 90% confident the center image is of a cat, while 9% confident the image is of a dog and only 1% confident that the image is of an airplane. This richer information, beyond the direct class predictions alone, is often termed “dark knowledge” <d-cite key="Hinton2015distilling"></d-cite>.</p> <h3 id="the-role-of-temperature-in-softmax">The Role of Temperature in Softmax</h3> <p>In classification, the raw outputs of a neural network (logits, $z$) are typically converted into probabilities using the softmax function. Knowledge distillation introduces a “temperature” parameter ($T$) into this softmax calculation:</p> \[p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}.\] <p>When $T=1$ (standard softmax), the output probabilities are often very sharp, with the correct class having a probability close to 1 and others close to 0 (a “hard” distribution). As $T$ increases, the probability distribution becomes “softer,” meaning the probabilities for incorrect classes become larger, revealing more of the teacher’s “dark knowledge” about class similarities.</p> <p>For example with a temperature of $T=1$, the softmax output for an input $\mathbf{x}$ might be a probability distribution over three classes (dog, cat, airplane):</p> \[\require{colorv2} \Large f(\textcolor{red}{\mathbf{x}}, T=1) = \{\textcolor{green}{0.09}, \textcolor{red}{0.9}, \textcolor{blue}{0.01}\},\] <p>while at a higher temperature of $T=10$, the output might be less confident in the its predictions:</p> \[\require{colorv2} \Large f(\textcolor{red}{\mathbf{x}}, T=10) = \{\textcolor{green}{0.4}, \textcolor{red}{0.5}, \textcolor{blue}{0.1}\}.\] <h3 id="the-distillation-process">The Distillation Process</h3> <p>In standard training, a student model learns by minimizing a cross-entropy loss based on the “hard” target labels. In knowledge distillation <d-cite key="Hinton2015distilling"></d-cite>, the student learns from two sources:</p> <ol> <li>The <strong>cross-entropy loss</strong> with the ground truth (“hard”) labels.</li> <li>A <strong>distillation loss</strong> (often Kullback-Leibler divergence) that encourages the student’s “soft” predictions (obtained using a higher temperature $T$) to match the teacher’s “soft” predictions (also obtained using temperature $T$).</li> </ol> <p>These two losses are typically combined using a weighting hyperparameter $\alpha$:</p> \[L_{KD} = \alpha L_{\textrm{distillation}} + (1 - \alpha) L_{\textrm{classification}},\] <p>where $L_{\textrm{classification}}$ is the cross-entropy loss with hard labels and $L_{\textrm{distillation}}$ is the distillation loss with soft labels.</p> <p>While in previous work the effect of $\alpha$ on fairness was studied <d-cite key="Chai2022fairness"></d-cite>, this work focuses on the effect of the distillation temperature $T$ on bias and fairness.</p> <h2 id="beyond-accuracy-does-the-student-learn-the-same-function">Beyond Accuracy: Does the Student Learn the Same Function?</h2> <div class="row bg-white"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_teachertostudentfunctions.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_teachertostudentfunctions.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Distillation of a smaller student from a larger teacher model can learn different functions." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>While knowledge distillation often maintains the overall generalization performance (test accuracy) of the teacher model <d-cite key="Hinton2015distilling"></d-cite>, a crucial question arises: Does this mean the student model has learned approximately the <em>same function</em> as the teacher?.</p> <p>The answer is: not necessarily. Accuracy is an aggregate measure over many samples. It’s possible for the student $g(\mathbf{x})$ to learn a different function than the teacher $f(\mathbf{x})$ while still achieving similar overall accuracy.</p> <p>This divergence matters because if the student learns a different function, it may also learn different <strong>algorithmic biases</strong> than the teacher, even if the original teacher model was carefully analyzed for fairness.</p> <h2 id="research-deep-dive-unpacking-the-impact-of-distillation">Research Deep Dive: Unpacking the Impact of Distillation</h2> <p>This concern prompted the research questions behind our work <d-cite key="Mohammadshahi2025distillation"></d-cite>:</p> <h3 id="research-questions">Research Questions</h3> <ol> <li>Which specific classes are significantly affected by the distillation process in terms of their accuracy?</li> <li>How does varying the distillation temperature ($T$) impact the class-level biases of the student model?</li> <li>What is the effect of distillation temperature on <strong>group fairness</strong> (ensuring equitable outcomes across different demographic groups)?</li> <li>How does distillation temperature influence <strong>individual fairness</strong> (ensuring similar individuals receive similar predictions)?</li> </ol> <h3 id="analyzing-class-wise-bias">Analyzing Class-wise Bias</h3> <div class="row align-items-center justify-content-center text-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_distilledvsnondistilledstudent.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_distilledvsnondistilledstudent.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Student vs. Non-Distilled Student" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption">Figure: In order to better understand the effect of Knowledge Distillation, and to control effects on bias/fairness of model size, we compared a Distilled Student (DS) to a Non-Distilled Student (NDS), i.e. a student trained with distillation from a teacher compared to a student model trained from random initialization with the same dataset.</div> </div> <p>To understand which classes are affected, we can compared model predictions across a dataset. They defined disagreement between two models, $f$ and $g$, for an input $\mathbf{x}_n$ using a comparison metric (CMP) similar to approaches in works like <d-cite key="Fort2019deepensembles"></d-cite>:</p> \[CMP(f(\mathbf{x}_n), g(\mathbf{x}_n)) = \begin{cases} 0 &amp; \text{if } f(\mathbf{x}_n) = g(\mathbf{x}_n) \\ 1 &amp; \text{if } f(\mathbf{x}_n) \neq g(\mathbf{x}_n) \end{cases}\] <p>This disagreement was analyzed on a per-class basis, comparing the (teacher vs. distilled student) and (non-distilled student vs. distilled student). A non-distilled student (trained from scratch on hard labels) served as a baseline.</p> <h3 id="probing-group-fairness">Probing Group Fairness</h3> <div class="container"> <div class="row align-items-center justify-content-center text-center bg-white"> <div class="col-lg mt-3 mt-md-0 align-items-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_demographicparity.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_demographicparity.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Demographic Parity" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-lg mt-3 mt-md-0 align-items-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_equalizedodds.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_equalizedodds.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Equalized Odds" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-lg mt-3 mt-md-0"> <div class="caption">Demographic Partity</div> </div> <div class="col-lg mt-3 mt-md-0"> <div class="caption">Equalized Odds</div> </div> </div> <div class="row text-center justify-content-center"> <div class="caption">Figure: Group fairness metrics used in our analysis.</div> </div> </div> <p>A more direct concern is when changes in model behavior lead to unfair outcomes for different demographic groups. The research <d-cite key="Mohammadshahi2025distillation"></d-cite> investigated two standard group fairness notions:</p> <ul> <li> <strong>Demographic Parity:</strong> Aims for the probability of a positive outcome ($Y=1$) to be the same across different sensitive groups $A=a$ and $A=b$ (e.g., men vs. women being hired).</li> </ul> \[P(\hat{Y}=1 | A=a) = P(\hat{Y}=1 | A=b)\] <p>This is often measured by the <strong>Demographic Parity Difference (DPD)</strong>, where DPD=0 indicates perfect fairness under this definition.</p> \[DPD = \max_{a \in A} P(\hat{Y}=1 | A=a) - \min_{a \in A} P(\hat{Y}=1 | A=a)\] <ul> <li> <strong>Equalized Odds</strong> <d-cite key="Hardt2016equality"></d-cite>: Aims for the true positive rate and false positive rate to be similar across different groups, given the true label $Y=y$ (e.g., qualified men and qualified women having equal hiring rates).</li> </ul> \[P(\hat{Y}=1 | Y=y, A=a) = P(\hat{Y}=1 | Y=y, A=b)\] <p>This is measured by the <strong>Equalized Odds Difference (EOD)</strong>, where EOD=0 is ideal.</p> <p>These metrics were evaluated on datasets with known demographic attributes:</p> <ul> <li> <strong>CelebA</strong> <d-cite key="Liu2015celeba"></d-cite>: Celebrity faces with attributes like gender and age, used for tasks like “smiling” prediction.</li> <li> <strong>Trifeature</strong> <d-cite key="Hermann2020shapes"></d-cite>: A synthetic dataset with controlled shapes, textures, and colors, used to isolate the effect of feature difficulty.</li> <li> <strong>HateXplain</strong> <d-cite key="Mathew2021hatexplain"></d-cite>: A dataset for hate speech detection, with annotations for targeted communities.</li> </ul> <h3 id="investigating-individual-fairness">Investigating Individual Fairness</h3> <p>Beyond group-level fairness, our study <d-cite key="Mohammadshahi2025distillation"></d-cite> also examined <strong>individual fairness</strong>: the principle that similar individuals should receive similar outcomes. This was quantified using a metric based on the Lipschitz condition proposed by Dwork et al. <d-cite key="Dwork2012fairness"></d-cite>, where smaller values indicate better individual fairness.</p> <h2 id="key-findings-and-insights">Key Findings and Insights</h2> <p>Our research <d-cite key="Mohammadshahi2025distillation"></d-cite> yielded several important findings regarding the interplay of knowledge distillation, temperature, and fairness.</p> <h3 id="class-wise-bias-an-uneven-impact">Class-wise Bias: An Uneven Impact</h3> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table: Class-wise Bias and Distillation. The number of statistically significantly affected classes comparing the class-wise accuracy of *teacher vs. Distilled Student (DS) models*, denoted #TC, and *Non-Distilled Student (NDS) vs. distilled student models*, denoted #SC for the ImageNet dataset. </div> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Temp</th> <th style="text-align: center">ResNet50/ResNet18</th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> <th style="text-align: center">ViT-Base/TinyViT</th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> </tr> <tr> <th style="text-align: left"> </th> <th style="text-align: left"> </th> <th style="text-align: center">Test Top-1 Acc. (%)</th> <th style="text-align: center">#SC</th> <th style="text-align: center">#TC</th> <th style="text-align: center">Test Top-1 Acc. (%)</th> <th style="text-align: center">#SC</th> <th style="text-align: center">#TC</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Teacher</td> <td style="text-align: left">-</td> <td style="text-align: center">76.1 ± 0.13</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">81.02 ± 0.07</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> </tr> <tr> <td style="text-align: left">NDS</td> <td style="text-align: left">-</td> <td style="text-align: center">68.64 ± 0.21</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">78.68 ± 0.19</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">2</td> <td style="text-align: center">68.93 ± 0.23</td> <td style="text-align: center">77</td> <td style="text-align: center">314</td> <td style="text-align: center">78.79 ± 0.21</td> <td style="text-align: center">83</td> <td style="text-align: center">397</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">3</td> <td style="text-align: center">69.12 ± 0.18</td> <td style="text-align: center">113</td> <td style="text-align: center">265</td> <td style="text-align: center">78.94 ± 0.14</td> <td style="text-align: center">137</td> <td style="text-align: center">318</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">4</td> <td style="text-align: center">69.57 ± 0.26</td> <td style="text-align: center">169</td> <td style="text-align: center">237</td> <td style="text-align: center">79.12 ± 0.23</td> <td style="text-align: center">186</td> <td style="text-align: center">253</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">5</td> <td style="text-align: center">69.85 ± 0.19</td> <td style="text-align: center">190</td> <td style="text-align: center">218</td> <td style="text-align: center">79.51 ± 0.17</td> <td style="text-align: center">215</td> <td style="text-align: center">206</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">6</td> <td style="text-align: center">69.71 ± 0.13</td> <td style="text-align: center">212</td> <td style="text-align: center">193</td> <td style="text-align: center">80.03 ± 0.19</td> <td style="text-align: center">268</td> <td style="text-align: center">184</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">7</td> <td style="text-align: center">70.05 ± 0.18</td> <td style="text-align: center">295</td> <td style="text-align: center">174</td> <td style="text-align: center">79.62 ± 0.23</td> <td style="text-align: center">329</td> <td style="text-align: center">161</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">8</td> <td style="text-align: center">70.28 ± 0.27</td> <td style="text-align: center">346</td> <td style="text-align: center">138</td> <td style="text-align: center">79.93 ± 0.12</td> <td style="text-align: center">365</td> <td style="text-align: center">127</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">9</td> <td style="text-align: center">70.52 ± 0.09</td> <td style="text-align: center">371</td> <td style="text-align: center">101</td> <td style="text-align: center">80.16 ± 0.17</td> <td style="text-align: center">397</td> <td style="text-align: center">96</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">10</td> <td style="text-align: center">70.83 ± 0.15</td> <td style="text-align: center">408</td> <td style="text-align: center">86</td> <td style="text-align: center">79.98 ± 0.12</td> <td style="text-align: center">426</td> <td style="text-align: center">78</td> </tr> </tbody> </table> </div> <div class="container"> <div class="row"> <div class="col-md mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fig2_a.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fig2_a.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="CIFAR-10 using T=9" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fig2_b.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fig2_b.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="SVHN using T=7" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure: Class-wise Disagreement. Disagreement between a ResNet-56 teacher and ResNet-20 (left) non-distilled/(right) distilled student for (a) CIFAR-10 using T= 9 and (b) SVHN using T= 7. The diagonals are excluded since here both models predict the same class without any disagreement. </div> </div> <p>Class-wise bias experiments were conducted across various datasets (CIFAR-10/100, SVHN, Tiny ImageNet, ImageNet) and model architectures (ResNets, ViTs) <d-cite key="Mohammadshahi2025distillation"></d-cite>. In order to understand the effect of distillation on a student model, we compared the distilled student model to both the teacher model and a non-distilled student model (trained from scratch on hard labels).</p> <p>Distillation does not affect all classes uniformly; a significant percentage of classes can experience changes in accuracy. The distillation temperature $T$ influences which model (teacher or non-distilled student) the distilled student’s biases more closely resemble. Higher temperatures tend to align the student more with the teacher’s class-specific performance patterns <d-cite key="Mohammadshahi2025distillation"></d-cite>.</p> <p>Our study <d-cite key="Mohammadshahi2025distillation"></d-cite> found that a change in class bias by itself isn’t inherently good or bad; its implications depend on the application context, leading to the analysis of the impact on decisions, i.e., group and individual fairness.</p> <h3 id="group-fairness-temperature-matters">Group Fairness: Temperature Matters</h3> <div class="container bg-white"> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_celeba_gender.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_celeba_gender.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_celeba_race.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_celeba_race.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_legend.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_legend.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Figure: Combined graphs showing EOD/DPD decreasing with increasing temperature for CelebA image dataset. </div> <div class="container bg-white"> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_hatexplain.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_hatexplain.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_legend.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_legend.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Figure: Combined graphs showing EOD/DPD decreasing with increasing temperature for the HateXplain language dataset. </div> <p>Across all three datasets (CelebA, Trifeature, HateXplain) and for both computer vision and NLP tasks, a consistent trend emerged <d-cite key="Mohammadshahi2025distillation"></d-cite>:</p> <ul> <li> <strong>Increasing the distillation temperature ($T$) generally leads to improved group fairness</strong> in the student model, as measured by lower DPD and EOD values.</li> <li>Remarkably, in some instances, the <strong>distilled student model (especially at higher temperatures) can become fairer than the original, larger teacher model</strong>.</li> </ul> <h4 id="very-high-temperatures">Very High Temperatures</h4> <div class="container bg-white"> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_hatexplain_all.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_hatexplain_all.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_legend.svg" sizes="95vw"></source> <img src="/spectrum-website/assets/img/distillation_fairness_legend.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> Figure: Combined/representative graphs showing EOD/DPD decreasing with very high temperatures for HateXplain. </div> <p>Of course at higher levels of temperature, the model’s predictions become more uniform, which can lead to a loss of accuracy. Our study found that while distillation a moderately high temperature (e.g., $T=10$) can lead to improved fairness, very high temperatures (e.g. $T&gt;10$) can lead to a significant drop in accuracy and fairness.</p> <h3 id="individual-fairness-consistency-improves">Individual Fairness: Consistency Improves</h3> <p>Similar to group fairness, our study <d-cite key="Mohammadshahi2025distillation"></d-cite> found a <strong>clear improvement in individual fairness with increased distillation temperature</strong> across the tested datasets. This suggests that higher temperatures not only help in equitable group outcomes but also in making the model’s predictions more consistent for similar inputs.</p> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table: Individual Fairness Metrics Across Datasets. Individual fairness scores for teacher, Non-Distilled Student (NDS), and Distilled Student (DS) models across CelebA, Trifeature, and HateXplain datasets. For DS models, scores are reported for varying temperature values $T$. </div> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Temp</th> <th style="text-align: center">CelebA (ResNet-50 / ResNet-18)</th> <th style="text-align: center">Trifeature (ResNet-20 / LeNet-5)</th> <th style="text-align: center">HateXplain (Bert-Base / DistilBERT)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Teacher</td> <td style="text-align: left">–</td> <td style="text-align: center">0.0407</td> <td style="text-align: center">0.016</td> <td style="text-align: center">0.0320</td> </tr> <tr> <td style="text-align: left">NDS</td> <td style="text-align: left">–</td> <td style="text-align: center">0.1240</td> <td style="text-align: center">0.0462</td> <td style="text-align: center">0.1078</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">1</td> <td style="text-align: center">0.1130</td> <td style="text-align: center">0.0422</td> <td style="text-align: center">0.0994</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">2</td> <td style="text-align: center">0.1040</td> <td style="text-align: center">0.0407</td> <td style="text-align: center">0.0985</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">3</td> <td style="text-align: center">0.0908</td> <td style="text-align: center">0.0393</td> <td style="text-align: center">0.0927</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">4</td> <td style="text-align: center">0.0906</td> <td style="text-align: center">0.0387</td> <td style="text-align: center">0.0882</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">5</td> <td style="text-align: center">0.0886</td> <td style="text-align: center">0.0384</td> <td style="text-align: center">0.0823</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">6</td> <td style="text-align: center">0.0799</td> <td style="text-align: center">0.0377</td> <td style="text-align: center">0.0768</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">7</td> <td style="text-align: center">0.0753</td> <td style="text-align: center">0.0356</td> <td style="text-align: center">0.0727</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">8</td> <td style="text-align: center">0.0712</td> <td style="text-align: center">0.0349</td> <td style="text-align: center">0.0689</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">9</td> <td style="text-align: center">0.0701</td> <td style="text-align: center">0.0341</td> <td style="text-align: center">0.0681</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">10</td> <td style="text-align: center">0.0697</td> <td style="text-align: center">0.0338</td> <td style="text-align: center">0.0654</td> </tr> </tbody> </table> </div> <h2 id="conclusion-distillation-a-double-edged-sword">Conclusion: Distillation, A Double-Edged Sword?</h2> <p>Knowledge distillation is a pervasive technique, likely affecting decisions made by models we interact with daily. This research <d-cite key="Mohammadshahi2025distillation"></d-cite> highlights that while KD is valuable for model compression, its effects are more nuanced than simply preserving accuracy.</p> <ul> <li>Distillation temperature significantly influences model bias and fairness across various models, datasets, and even modalities (vision and language).</li> <li>Higher distillation temperatures tend to produce fairer student models, sometimes even surpassing the teacher in fairness metrics.</li> </ul> <p>This is a critical finding, as the effect of distillation temperature on fairness had not been extensively studied before <d-cite key="Mohammadshahi2025distillation"></d-cite>.</p> <h2 id="future-directions">Future Directions</h2> <p>These findings <d-cite key="Mohammadshahi2025distillation"></d-cite> open up several avenues for future investigation:</p> <ul> <li>Can knowledge distillation, particularly with careful tuning of temperature, be actively used as a method to <em>improve</em> model fairness?</li> <li>What are the trade-offs involved when using higher distillation temperatures, which are less common in current practice focused primarily on accuracy? Does it affect other aspects like robustness or calibration?</li> <li>How do these fairness dynamics play out in the context of even larger models, such as modern Large Language Models (LLMs) like DeepSeek <d-cite key="DeepSeek2024v3"></d-cite>?</li> </ul> <p>Understanding these aspects will be crucial for the responsible development and deployment of distilled AI models.</p> <h2 id="citing-our-work">Citing our work</h2> <p>If you find this work useful, please consider citing it using the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">mohammadshahi2025leftafterdistillation</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mohammadshahi, Aida and Ioannou, Yani}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">arxivid</span> <span class="p">=</span> <span class="s">{2410.08407}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2410.08407}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span>
<span class="p">}</span>
</code></pre></div></div> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">✨Our paper is now officially published in Transactions on Machine Learning Research (TMLR)!<br><br>We explore how knowledge <a href="https://twitter.com/hashtag/distillation?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#distillation</a> (KD) impacts fairness &amp; bias in AI models, across both group and individual fairness. <a href="https://t.co/9V2czrYHsg" rel="external nofollow noopener" target="_blank">pic.twitter.com/9V2czrYHsg</a></p>— Aida Mohammadshahi (@Aidamo27) <a href="https://twitter.com/Aidamo27/status/1912626418867196160?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">April 16, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/spectrum-website/assets/bibliography/2025-05-16-distillation.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Spectrum Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/calgaryml/calgaryml.github.io" rel="external nofollow noopener" target="_blank">customized theme</a> based on <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/spectrum-website/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/spectrum-website/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>