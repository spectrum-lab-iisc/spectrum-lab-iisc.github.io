<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://spectrum-lab-iisc.github.io/spectrum-website/feed.xml" rel="self" type="application/atom+xml"/><link href="https://spectrum-lab-iisc.github.io/spectrum-website/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-11T10:07:05+00:00</updated><id>https://spectrum-lab-iisc.github.io/spectrum-website/feed.xml</id><subtitle>Spectrum Lab</subtitle><entry><title type="html">Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models</title><link href="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2025/distillation-and-fairness/" rel="alternate" type="text/html" title="Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models"/><published>2025-03-31T00:00:00+00:00</published><updated>2025-03-31T00:00:00+00:00</updated><id>https://spectrum-lab-iisc.github.io/spectrum-website/blog/2025/distillation-and-fairness</id><content type="html" xml:base="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2025/distillation-and-fairness/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>Knowledge Distillation (or distillation) is a technique used to compress large AI models into smaller, more efficient versions. For example, DeepSeek R1 with 671 billion parameters <d-cite key="DeepSeek2024v3"></d-cite>, was distilled into smaller, more manageable versions that are easier to deploy in real-world applications.</p> <p>While distillation often succeeds in maintaining overall accuracy, our recently accepted Transactions in Machine Learning Research (TMLR) paper, “<a href="https://openreview.net/pdf?id=xBbj46Y2fN">What’s Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias</a>” <d-cite key="Mohammadshahi2025distillation"></d-cite> explores how the distillation process affects model decisions, particularly in terms of fairness and bias. We found that:</p> <ul> <li>The distillation temperature significantly influences the biases of the student model relative to the teacher model, and a smaller student model trained from scratch.</li> <li>Higher distillation temperatures generally lead to distilled models that make more fair decisions, i.e. improved group fairness and individual fairness metrics.</li> <li>Surprisingly, distilled models trained at high temperatures rarely used in practice, e.g. $T=10$, can be fairer than their larger teacher counterparts.</li> <li>This research highlights the need to consider fairness implications when using distillation, especially in sensitive applications where impactful decisions are made, like hiring or loan approvals.</li> </ul> <h2 id="introduction-knowledge-distillation">Introduction: Knowledge Distillation</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_teachertostudent_simple.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_teachertostudent_simple.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Distillation of a smaller student from a larger teacher model." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Large models, like DeepSeek R1 with 671 billion parameters <d-cite key="DeepSeek2024v3"></d-cite>, are often distilled into smaller, more manageable versions (e.g., 1.5-70B Llama models) that are easier to deploy in real-world applications. This process, known as Knowledge Distillation (or just distillation) <d-cite key="Hinton2015distilling"></d-cite>, aims to transfer the “knowledge” from a large “teacher” model to a smaller “student” model, often preserving overall performance like test accuracy.</p> <p>While distillation often succeeds in maintaining overall accuracy, our recenly accepted Transactions in Machine Learning Researc (TMLR) paper, “<a href="https://openreview.net/forum?id=xBbj46Y2fN">What’s Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias</a>” <d-cite key="Mohammadshahi2025distillation"></d-cite>, takes a deeper dive into understanding how distillation affects the decisions made by a model, through the lens of fairness and bias. This is particularly important as AI systems are increasingly used in sensitive areas like hiring, loan applications, and medical diagnosis, where fairness is crucial.</p> <p><strong>Does the distilled student model treat all groups and types of data the same way the teacher did, or does the process introduce new, potentially harmful, biases?</strong> To grasp the implications of KD, let’s first revisit some core concepts.</p> <h2 id="understanding-knowledge-distillation">Understanding Knowledge Distillation:</h2> <h3 id="neural-networks-as-function-approximators">Neural Networks as Function Approximators</h3> <p>At their heart, neural networks are powerful function approximators. They learn a function $f$ that maps an input $\mathbf{x}$ to an output $y$ (or a probability distribution $p$ over possible outputs in classification tasks),</p> \[f(\mathbf{x}) = y ,\] <p>where $f$ is the model, $\mathbf{x}$ is the input (like an image or text), and $y$ is the output (like a label or a probability distribution). The goal of training is to minimize the difference between the model’s predictions and the true labels, often using a loss function like cross-entropy.</p> <h3 id="the-concept-of-dark-knowledge">The Concept of “Dark Knowledge”</h3> <div class="row align-items-center"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_softtargets_catdogairplane.png" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_softtargets_catdogairplane.png" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row align-item-center justify-content-center"> \[\require{colorv2} \Large f(\textcolor{red}{\mathbf{x} \textrm{: image of cat}}) = \{ \textcolor{green}{\textrm{dog: } 0.09}, \textcolor{red}{\textrm{cat: } 0.9}, \textcolor{blue}{\textrm{airplane: } 0.01}\}\] </div> <p>Trained models, especially large ones, learn much more than just how to map inputs to correct labels. They capture a rich, nuanced understanding of the data’s structure and relationships. For example, an ImageNet model doesn’t just learn to identify a “cat”; it also implicitly learns that a cat is more similar to a “dog” than to an “airplane”. In this example, the model is 90% confident the center image is of a cat, while 9% confident the image is of a dog and only 1% confident that the image is of an airplane. This richer information, beyond the direct class predictions alone, is often termed “dark knowledge” <d-cite key="Hinton2015distilling"></d-cite>.</p> <h3 id="the-role-of-temperature-in-softmax">The Role of Temperature in Softmax</h3> <p>In classification, the raw outputs of a neural network (logits, $z$) are typically converted into probabilities using the softmax function. Knowledge distillation introduces a “temperature” parameter ($T$) into this softmax calculation:</p> \[p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}.\] <p>When $T=1$ (standard softmax), the output probabilities are often very sharp, with the correct class having a probability close to 1 and others close to 0 (a “hard” distribution). As $T$ increases, the probability distribution becomes “softer,” meaning the probabilities for incorrect classes become larger, revealing more of the teacher’s “dark knowledge” about class similarities.</p> <p>For example with a temperature of $T=1$, the softmax output for an input $\mathbf{x}$ might be a probability distribution over three classes (dog, cat, airplane):</p> \[\require{colorv2} \Large f(\textcolor{red}{\mathbf{x}}, T=1) = \{\textcolor{green}{0.09}, \textcolor{red}{0.9}, \textcolor{blue}{0.01}\},\] <p>while at a higher temperature of $T=10$, the output might be less confident in the its predictions:</p> \[\require{colorv2} \Large f(\textcolor{red}{\mathbf{x}}, T=10) = \{\textcolor{green}{0.4}, \textcolor{red}{0.5}, \textcolor{blue}{0.1}\}.\] <h3 id="the-distillation-process">The Distillation Process</h3> <p>In standard training, a student model learns by minimizing a cross-entropy loss based on the “hard” target labels. In knowledge distillation <d-cite key="Hinton2015distilling"></d-cite>, the student learns from two sources:</p> <ol> <li>The <strong>cross-entropy loss</strong> with the ground truth (“hard”) labels.</li> <li>A <strong>distillation loss</strong> (often Kullback-Leibler divergence) that encourages the student’s “soft” predictions (obtained using a higher temperature $T$) to match the teacher’s “soft” predictions (also obtained using temperature $T$).</li> </ol> <p>These two losses are typically combined using a weighting hyperparameter $\alpha$:</p> \[L_{KD} = \alpha L_{\textrm{distillation}} + (1 - \alpha) L_{\textrm{classification}},\] <p>where $L_{\textrm{classification}}$ is the cross-entropy loss with hard labels and $L_{\textrm{distillation}}$ is the distillation loss with soft labels.</p> <p>While in previous work the effect of $\alpha$ on fairness was studied <d-cite key="Chai2022fairness"></d-cite>, this work focuses on the effect of the distillation temperature $T$ on bias and fairness.</p> <h2 id="beyond-accuracy-does-the-student-learn-the-same-function">Beyond Accuracy: Does the Student Learn the Same Function?</h2> <div class="row bg-white"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_teachertostudentfunctions.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_teachertostudentfunctions.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Distillation of a smaller student from a larger teacher model can learn different functions." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While knowledge distillation often maintains the overall generalization performance (test accuracy) of the teacher model <d-cite key="Hinton2015distilling"></d-cite>, a crucial question arises: Does this mean the student model has learned approximately the <em>same function</em> as the teacher?.</p> <p>The answer is: not necessarily. Accuracy is an aggregate measure over many samples. It’s possible for the student $g(\mathbf{x})$ to learn a different function than the teacher $f(\mathbf{x})$ while still achieving similar overall accuracy.</p> <p>This divergence matters because if the student learns a different function, it may also learn different <strong>algorithmic biases</strong> than the teacher, even if the original teacher model was carefully analyzed for fairness.</p> <h2 id="research-deep-dive-unpacking-the-impact-of-distillation">Research Deep Dive: Unpacking the Impact of Distillation</h2> <p>This concern prompted the research questions behind our work <d-cite key="Mohammadshahi2025distillation"></d-cite>:</p> <h3 id="research-questions">Research Questions</h3> <ol> <li>Which specific classes are significantly affected by the distillation process in terms of their accuracy?</li> <li>How does varying the distillation temperature ($T$) impact the class-level biases of the student model?</li> <li>What is the effect of distillation temperature on <strong>group fairness</strong> (ensuring equitable outcomes across different demographic groups)?</li> <li>How does distillation temperature influence <strong>individual fairness</strong> (ensuring similar individuals receive similar predictions)?</li> </ol> <h3 id="analyzing-class-wise-bias">Analyzing Class-wise Bias</h3> <div class="row align-items-center justify-content-center text-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_distilledvsnondistilledstudent.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_distilledvsnondistilledstudent.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Student vs. Non-Distilled Student" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption">Figure: In order to better understand the effect of Knowledge Distillation, and to control effects on bias/fairness of model size, we compared a Distilled Student (DS) to a Non-Distilled Student (NDS), i.e. a student trained with distillation from a teacher compared to a student model trained from random initialization with the same dataset.</div> </div> <p>To understand which classes are affected, we can compared model predictions across a dataset. They defined disagreement between two models, $f$ and $g$, for an input $\mathbf{x}_n$ using a comparison metric (CMP) similar to approaches in works like <d-cite key="Fort2019deepensembles"></d-cite>:</p> \[CMP(f(\mathbf{x}_n), g(\mathbf{x}_n)) = \begin{cases} 0 &amp; \text{if } f(\mathbf{x}_n) = g(\mathbf{x}_n) \\ 1 &amp; \text{if } f(\mathbf{x}_n) \neq g(\mathbf{x}_n) \end{cases}\] <p>This disagreement was analyzed on a per-class basis, comparing the (teacher vs. distilled student) and (non-distilled student vs. distilled student). A non-distilled student (trained from scratch on hard labels) served as a baseline.</p> <h3 id="probing-group-fairness">Probing Group Fairness</h3> <div class="container"> <div class="row align-items-center justify-content-center text-center bg-white"> <div class="col-lg mt-3 mt-md-0 align-items-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_demographicparity.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_demographicparity.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Demographic Parity" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-lg mt-3 mt-md-0 align-items-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_equalizedodds.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_equalizedodds.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Equalized Odds" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-lg mt-3 mt-md-0"> <div class="caption">Demographic Partity</div> </div> <div class="col-lg mt-3 mt-md-0"> <div class="caption">Equalized Odds</div> </div> </div> <div class="row text-center justify-content-center"> <div class="caption">Figure: Group fairness metrics used in our analysis.</div> </div> </div> <p>A more direct concern is when changes in model behavior lead to unfair outcomes for different demographic groups. The research <d-cite key="Mohammadshahi2025distillation"></d-cite> investigated two standard group fairness notions:</p> <ul> <li><strong>Demographic Parity:</strong> Aims for the probability of a positive outcome ($Y=1$) to be the same across different sensitive groups $A=a$ and $A=b$ (e.g., men vs. women being hired).</li> </ul> \[P(\hat{Y}=1 | A=a) = P(\hat{Y}=1 | A=b)\] <p>This is often measured by the <strong>Demographic Parity Difference (DPD)</strong>, where DPD=0 indicates perfect fairness under this definition.</p> \[DPD = \max_{a \in A} P(\hat{Y}=1 | A=a) - \min_{a \in A} P(\hat{Y}=1 | A=a)\] <ul> <li><strong>Equalized Odds</strong> <d-cite key="Hardt2016equality"></d-cite>: Aims for the true positive rate and false positive rate to be similar across different groups, given the true label $Y=y$ (e.g., qualified men and qualified women having equal hiring rates).</li> </ul> \[P(\hat{Y}=1 | Y=y, A=a) = P(\hat{Y}=1 | Y=y, A=b)\] <p>This is measured by the <strong>Equalized Odds Difference (EOD)</strong>, where EOD=0 is ideal.</p> <p>These metrics were evaluated on datasets with known demographic attributes:</p> <ul> <li><strong>CelebA</strong> <d-cite key="Liu2015celeba"></d-cite>: Celebrity faces with attributes like gender and age, used for tasks like “smiling” prediction.</li> <li><strong>Trifeature</strong> <d-cite key="Hermann2020shapes"></d-cite>: A synthetic dataset with controlled shapes, textures, and colors, used to isolate the effect of feature difficulty.</li> <li><strong>HateXplain</strong> <d-cite key="Mathew2021hatexplain"></d-cite>: A dataset for hate speech detection, with annotations for targeted communities.</li> </ul> <h3 id="investigating-individual-fairness">Investigating Individual Fairness</h3> <p>Beyond group-level fairness, our study <d-cite key="Mohammadshahi2025distillation"></d-cite> also examined <strong>individual fairness</strong>: the principle that similar individuals should receive similar outcomes. This was quantified using a metric based on the Lipschitz condition proposed by Dwork et al. <d-cite key="Dwork2012fairness"></d-cite>, where smaller values indicate better individual fairness.</p> <h2 id="key-findings-and-insights">Key Findings and Insights</h2> <p>Our research <d-cite key="Mohammadshahi2025distillation"></d-cite> yielded several important findings regarding the interplay of knowledge distillation, temperature, and fairness.</p> <h3 id="class-wise-bias-an-uneven-impact">Class-wise Bias: An Uneven Impact</h3> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table: Class-wise Bias and Distillation. The number of statistically significantly affected classes comparing the class-wise accuracy of *teacher vs. Distilled Student (DS) models*, denoted #TC, and *Non-Distilled Student (NDS) vs. distilled student models*, denoted #SC for the ImageNet dataset. </div> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Temp</th> <th style="text-align: center">ResNet50/ResNet18</th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> <th style="text-align: center">ViT-Base/TinyViT</th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> </tr> <tr> <th style="text-align: left"> </th> <th style="text-align: left"> </th> <th style="text-align: center">Test Top-1 Acc. (%)</th> <th style="text-align: center">#SC</th> <th style="text-align: center">#TC</th> <th style="text-align: center">Test Top-1 Acc. (%)</th> <th style="text-align: center">#SC</th> <th style="text-align: center">#TC</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Teacher</td> <td style="text-align: left">-</td> <td style="text-align: center">76.1 ± 0.13</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">81.02 ± 0.07</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> </tr> <tr> <td style="text-align: left">NDS</td> <td style="text-align: left">-</td> <td style="text-align: center">68.64 ± 0.21</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">78.68 ± 0.19</td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">2</td> <td style="text-align: center">68.93 ± 0.23</td> <td style="text-align: center">77</td> <td style="text-align: center">314</td> <td style="text-align: center">78.79 ± 0.21</td> <td style="text-align: center">83</td> <td style="text-align: center">397</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">3</td> <td style="text-align: center">69.12 ± 0.18</td> <td style="text-align: center">113</td> <td style="text-align: center">265</td> <td style="text-align: center">78.94 ± 0.14</td> <td style="text-align: center">137</td> <td style="text-align: center">318</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">4</td> <td style="text-align: center">69.57 ± 0.26</td> <td style="text-align: center">169</td> <td style="text-align: center">237</td> <td style="text-align: center">79.12 ± 0.23</td> <td style="text-align: center">186</td> <td style="text-align: center">253</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">5</td> <td style="text-align: center">69.85 ± 0.19</td> <td style="text-align: center">190</td> <td style="text-align: center">218</td> <td style="text-align: center">79.51 ± 0.17</td> <td style="text-align: center">215</td> <td style="text-align: center">206</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">6</td> <td style="text-align: center">69.71 ± 0.13</td> <td style="text-align: center">212</td> <td style="text-align: center">193</td> <td style="text-align: center">80.03 ± 0.19</td> <td style="text-align: center">268</td> <td style="text-align: center">184</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">7</td> <td style="text-align: center">70.05 ± 0.18</td> <td style="text-align: center">295</td> <td style="text-align: center">174</td> <td style="text-align: center">79.62 ± 0.23</td> <td style="text-align: center">329</td> <td style="text-align: center">161</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">8</td> <td style="text-align: center">70.28 ± 0.27</td> <td style="text-align: center">346</td> <td style="text-align: center">138</td> <td style="text-align: center">79.93 ± 0.12</td> <td style="text-align: center">365</td> <td style="text-align: center">127</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">9</td> <td style="text-align: center">70.52 ± 0.09</td> <td style="text-align: center">371</td> <td style="text-align: center">101</td> <td style="text-align: center">80.16 ± 0.17</td> <td style="text-align: center">397</td> <td style="text-align: center">96</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">10</td> <td style="text-align: center">70.83 ± 0.15</td> <td style="text-align: center">408</td> <td style="text-align: center">86</td> <td style="text-align: center">79.98 ± 0.12</td> <td style="text-align: center">426</td> <td style="text-align: center">78</td> </tr> </tbody> </table> </div> <div class="container"> <div class="row"> <div class="col-md mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fig2_a.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fig2_a.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="CIFAR-10 using T=9" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fig2_b.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fig2_b.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="SVHN using T=7" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure: Class-wise Disagreement. Disagreement between a ResNet-56 teacher and ResNet-20 (left) non-distilled/(right) distilled student for (a) CIFAR-10 using T= 9 and (b) SVHN using T= 7. The diagonals are excluded since here both models predict the same class without any disagreement. </div> </div> <p>Class-wise bias experiments were conducted across various datasets (CIFAR-10/100, SVHN, Tiny ImageNet, ImageNet) and model architectures (ResNets, ViTs) <d-cite key="Mohammadshahi2025distillation"></d-cite>. In order to understand the effect of distillation on a student model, we compared the distilled student model to both the teacher model and a non-distilled student model (trained from scratch on hard labels).</p> <p>Distillation does not affect all classes uniformly; a significant percentage of classes can experience changes in accuracy. The distillation temperature $T$ influences which model (teacher or non-distilled student) the distilled student’s biases more closely resemble. Higher temperatures tend to align the student more with the teacher’s class-specific performance patterns <d-cite key="Mohammadshahi2025distillation"></d-cite>.</p> <p>Our study <d-cite key="Mohammadshahi2025distillation"></d-cite> found that a change in class bias by itself isn’t inherently good or bad; its implications depend on the application context, leading to the analysis of the impact on decisions, i.e., group and individual fairness.</p> <h3 id="group-fairness-temperature-matters">Group Fairness: Temperature Matters</h3> <div class="container bg-white"> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_celeba_gender.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_celeba_gender.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_celeba_race.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_celeba_race.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_legend.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_legend.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Figure: Combined graphs showing EOD/DPD decreasing with increasing temperature for CelebA image dataset. </div> <div class="container bg-white"> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_hatexplain.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_hatexplain.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_legend.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_legend.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Figure: Combined graphs showing EOD/DPD decreasing with increasing temperature for the HateXplain language dataset. </div> <p>Across all three datasets (CelebA, Trifeature, HateXplain) and for both computer vision and NLP tasks, a consistent trend emerged <d-cite key="Mohammadshahi2025distillation"></d-cite>:</p> <ul> <li><strong>Increasing the distillation temperature ($T$) generally leads to improved group fairness</strong> in the student model, as measured by lower DPD and EOD values.</li> <li>Remarkably, in some instances, the <strong>distilled student model (especially at higher temperatures) can become fairer than the original, larger teacher model</strong>.</li> </ul> <h4 id="very-high-temperatures">Very High Temperatures</h4> <div class="container bg-white"> <div class="row"> <div class="col-xl mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_hatexplain_all.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_hatexplain_all.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/distillation_fairness_legend.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/distillation_fairness_legend.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Figure: Combined/representative graphs showing EOD/DPD decreasing with very high temperatures for HateXplain. </div> <p>Of course at higher levels of temperature, the model’s predictions become more uniform, which can lead to a loss of accuracy. Our study found that while distillation a moderately high temperature (e.g., $T=10$) can lead to improved fairness, very high temperatures (e.g. $T&gt;10$) can lead to a significant drop in accuracy and fairness.</p> <h3 id="individual-fairness-consistency-improves">Individual Fairness: Consistency Improves</h3> <p>Similar to group fairness, our study <d-cite key="Mohammadshahi2025distillation"></d-cite> found a <strong>clear improvement in individual fairness with increased distillation temperature</strong> across the tested datasets. This suggests that higher temperatures not only help in equitable group outcomes but also in making the model’s predictions more consistent for similar inputs.</p> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table: Individual Fairness Metrics Across Datasets. Individual fairness scores for teacher, Non-Distilled Student (NDS), and Distilled Student (DS) models across CelebA, Trifeature, and HateXplain datasets. For DS models, scores are reported for varying temperature values $T$. </div> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Temp</th> <th style="text-align: center">CelebA (ResNet-50 / ResNet-18)</th> <th style="text-align: center">Trifeature (ResNet-20 / LeNet-5)</th> <th style="text-align: center">HateXplain (Bert-Base / DistilBERT)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Teacher</td> <td style="text-align: left">–</td> <td style="text-align: center">0.0407</td> <td style="text-align: center">0.016</td> <td style="text-align: center">0.0320</td> </tr> <tr> <td style="text-align: left">NDS</td> <td style="text-align: left">–</td> <td style="text-align: center">0.1240</td> <td style="text-align: center">0.0462</td> <td style="text-align: center">0.1078</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">1</td> <td style="text-align: center">0.1130</td> <td style="text-align: center">0.0422</td> <td style="text-align: center">0.0994</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">2</td> <td style="text-align: center">0.1040</td> <td style="text-align: center">0.0407</td> <td style="text-align: center">0.0985</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">3</td> <td style="text-align: center">0.0908</td> <td style="text-align: center">0.0393</td> <td style="text-align: center">0.0927</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">4</td> <td style="text-align: center">0.0906</td> <td style="text-align: center">0.0387</td> <td style="text-align: center">0.0882</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">5</td> <td style="text-align: center">0.0886</td> <td style="text-align: center">0.0384</td> <td style="text-align: center">0.0823</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">6</td> <td style="text-align: center">0.0799</td> <td style="text-align: center">0.0377</td> <td style="text-align: center">0.0768</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">7</td> <td style="text-align: center">0.0753</td> <td style="text-align: center">0.0356</td> <td style="text-align: center">0.0727</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">8</td> <td style="text-align: center">0.0712</td> <td style="text-align: center">0.0349</td> <td style="text-align: center">0.0689</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">9</td> <td style="text-align: center">0.0701</td> <td style="text-align: center">0.0341</td> <td style="text-align: center">0.0681</td> </tr> <tr> <td style="text-align: left">DS</td> <td style="text-align: left">10</td> <td style="text-align: center">0.0697</td> <td style="text-align: center">0.0338</td> <td style="text-align: center">0.0654</td> </tr> </tbody> </table> </div> <h2 id="conclusion-distillation-a-double-edged-sword">Conclusion: Distillation, A Double-Edged Sword?</h2> <p>Knowledge distillation is a pervasive technique, likely affecting decisions made by models we interact with daily. This research <d-cite key="Mohammadshahi2025distillation"></d-cite> highlights that while KD is valuable for model compression, its effects are more nuanced than simply preserving accuracy.</p> <ul> <li>Distillation temperature significantly influences model bias and fairness across various models, datasets, and even modalities (vision and language).</li> <li>Higher distillation temperatures tend to produce fairer student models, sometimes even surpassing the teacher in fairness metrics.</li> </ul> <p>This is a critical finding, as the effect of distillation temperature on fairness had not been extensively studied before <d-cite key="Mohammadshahi2025distillation"></d-cite>.</p> <h2 id="future-directions">Future Directions</h2> <p>These findings <d-cite key="Mohammadshahi2025distillation"></d-cite> open up several avenues for future investigation:</p> <ul> <li>Can knowledge distillation, particularly with careful tuning of temperature, be actively used as a method to <em>improve</em> model fairness?</li> <li>What are the trade-offs involved when using higher distillation temperatures, which are less common in current practice focused primarily on accuracy? Does it affect other aspects like robustness or calibration?</li> <li>How do these fairness dynamics play out in the context of even larger models, such as modern Large Language Models (LLMs) like DeepSeek <d-cite key="DeepSeek2024v3"></d-cite>?</li> </ul> <p>Understanding these aspects will be crucial for the responsible development and deployment of distilled AI models.</p> <h2 id="citing-our-work">Citing our work</h2> <p>If you find this work useful, please consider citing it using the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">mohammadshahi2025leftafterdistillation</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mohammadshahi, Aida and Ioannou, Yani}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">arxivid</span> <span class="p">=</span> <span class="s">{2410.08407}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2410.08407}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span>
<span class="p">}</span>
</code></pre></div></div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">✨Our paper is now officially published in Transactions on Machine Learning Research (TMLR)!<br/><br/>We explore how knowledge <a href="https://twitter.com/hashtag/distillation?src=hash&amp;ref_src=twsrc%5Etfw">#distillation</a> (KD) impacts fairness &amp; bias in AI models, across both group and individual fairness. <a href="https://t.co/9V2czrYHsg">pic.twitter.com/9V2czrYHsg</a></p>&mdash; Aida Mohammadshahi (@Aidamo27) <a href="https://twitter.com/Aidamo27/status/1912626418867196160?ref_src=twsrc%5Etfw">April 16, 2025</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div>]]></content><author><name>Aida Mohammadshahi</name></author><category term="distillation"/><category term="fairness"/><category term="bias"/><category term="compression"/><summary type="html"><![CDATA[A summary of our research exploring the effects of knowledge distillation on how deep neural networks make decisions, particularly in terms of fairness and bias.]]></summary></entry><entry><title type="html">Dynamic Sparse Training with Structured Sparsity</title><link href="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2024/training-sparse-structured-nn/" rel="alternate" type="text/html" title="Dynamic Sparse Training with Structured Sparsity"/><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://spectrum-lab-iisc.github.io/spectrum-website/blog/2024/training-sparse-structured-nn</id><content type="html" xml:base="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2024/training-sparse-structured-nn/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>The challenge in training sparse neural networks is to achieve both high accuracy and practical hardware acceleration. Unstructured sparsity often yields good performance but is hard to speed up, while traditional structured sparsity can hurt performance.</p> <p>Our International Conference in Learning Representations (ICLR) 2024 paper, <a href="https://openreview.net/forum?id=kOBkxFRKTA">“Dynamic Sparse Training with Structured Sparsity”</a> <d-cite key="Lasby2024SRigL"></d-cite> introduces Structured RigL (SRigL) to addresses this by dynamically learning hardware-friendly sparse weight representations without sacrificing accuracy. Key findings of the work include:</p> <ul> <li>SRigL successfully learns a combination of fine-grained N:M structured sparsity (constant fan-in) and neuron-level sparsity (neuron ablation) dynamically from a sparse initialization.</li> <li>The explicit integration of neuron ablation, a behavior implicitly learned by unstructured DST methods at high sparsities, is crucial for SRigL to match the generalization performance of dense and unstructured sparse models, even at extreme sparsities (up to 99%).</li> <li>The learned structured sparsity enables a “condensed sparse representation,” which translates to significant real-world inference speedups on commodity CPUs (up to $3.4 \times$ vs. dense at 90% sparsity) and GPUs (up to $1.7 \times$ vs. dense at 90% sparsity), outperforming unstructured sparse formats in many practical scenarios.</li> <li>SRigL demonstrates a viable path to train sparse neural networks that are both highly accurate and practically efficient, bridging the gap between unstructured DST performance and structured sparsity acceleration.</li> </ul> <h2 id="the-quest-for-more-efficient-neural-networks">The Quest for More Efficient Neural Networks</h2> <p>State-of-the-art deep neural networks (DNNs) have achieved remarkable feats, but their ever-increasing size brings ballooning training costs, often outstripping Moore’s Law. This trend makes cutting-edge AI research less accessible. While techniques exist to prune trained dense models, effectively reducing their parameter count by 85-95% without sacrificing generalization, they relay on dense pre-training. Can we train these sparse, efficient networks without dense training?</p> <h3 id="why-sparse-neural-networks">Why Sparse Neural Networks?</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-8 mt-3 mt-md-0"> Sparse neural networks offer several compelling advantages: <ul> <li>For a fixed number of weights, they can offer better generalization and fewer Floating Point Operations (FLOPs) at inference.</li> <li>They hold the potential to significantly reduce the computational cost of training.</li> <li>They provide a way to learn the inherent structure of neural networks, moving beyond a "one-size-fits-all" dense architecture.</li> </ul> </div> <div class="col-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsenn_simple.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsenn_simple.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Sparse Neural Network." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <h2 id="paths-to-sparsity-pruning-lottery-tickets-and-dynamic-sparse-training">Paths to Sparsity: Pruning, Lottery Tickets, and Dynamic Sparse Training</h2> <p>Several approaches have been developed to tackle the challenge of obtaining sparse networks, these include:</p> <h3 id="traditional-pruning-effective-but-costly">Traditional Pruning: Effective but Costly</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_pruning.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_pruning.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Pruning is highly effective, but requires dense pre-training." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Pruning Dense Trained Models: Pruning a trained model is highly effective, but requires pre-trained models.</div> </div> <p>Standard pruning techniques involve training a full, dense network and then removing (pruning) weights deemed less important, typically those with the smallest magnitude. This can be done once (“one-shot pruning”) or iteratively. While effective at finding highly sparse subnetworks that retain accuracy, this still necessitates the expensive initial dense training.</p> <h3 id="the-sparse-training-problem-training-with-a-fixed-sparse-mask">The Sparse Training Problem: Training with a Fixed Sparse Mask</h3> <div class="container"> <div class="row align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_problem.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_problem.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="The sparse training problem." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Sparse Training Problem: Training a sparse mask from random initialization doesn't recover the generalization of a pruned dense model, even for a known good mask.</div> </div> <p>Despite the success of pruning, simply training a sparse network from a random initialization, even with a known “good” sparse mask (the pattern of zeroed-out weights), often leads to poor performance compared to its dense counterpart or a pruned dense model. This is known as the sparse training problem.</p> <p>The Lottery Ticket Hypothesis (LTH) <d-cite key="Frankle2019LTH"></d-cite> proposed that within a large, randomly initialized dense network, there exist smaller subnetworks (the “winning tickets”) that, when trained in isolation from their original initialization weights (or weights from very early in training <d-cite key="Frankle2020LinearMode"></d-cite>), can achieve accuracy comparable to the full dense network. This was a foundational idea, suggesting that specific initializations within a sparse structure are crucial. However, finding these “winning tickets” is computationally expensive, especially for larger models, and the original findings were primarily on smaller datasets <d-cite key="Liu2019RethinkingPruning"></d-cite>. Research further showed that these Lottery Tickets often end up re-learning the solution that would have been found by pruning the dense model they originated from <d-cite key="Evci2022GradientFlow"></d-cite>.</p> <h3 id="dynamic-sparse-training-dst-training-sparse-from-the-start">Dynamic Sparse Training (DST): Training Sparse from the Start</h3> <div class="container"> <div class="row align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_dst.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_dst.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Dynamic Sparse Training (DST) methods are an alternative sparse training method that work well." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Dynamic Sparse Training (DST) methods are an alternative sparse training, where the mask is changed over the course of training. DST methods can match the generalization performance of standard dense training.</div> </div> <p>Dynamic Sparse Training (DST) methods offer a more direct approach to training sparse networks. Techniques like Sparse Evolutionary Training (SET) <d-cite key="Mocanu2018SET"></d-cite> and Rigging the Lottery Ticket (RigL) <d-cite key="Evci2020RigL"></d-cite> train networks that are sparse from initialization to the final solution (“sparse-to-sparse”). They achieve this by dynamically changing the sparse connectivity during training: periodically pruning less salient connections (e.g., small magnitude weights) and growing new ones (e.g., where the gradient magnitude is large). DST can achieve generalization comparable to dense training at high sparsity levels.</p> <h2 id="unstructured-vs-structured-sparsity">Unstructured vs. Structured Sparsity</h2> <div class="container"> <div class="row align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_unstructuredvsstructured.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_unstructuredvsstructured.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Unstructured vs. Structured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Unstructured vs. Structured sparsity. Unstructured sparsity is difficult to use efficiently on current computational hardware, such as CPUs and GPUs.</div> </div> <h3 id="unstructured-sparsity">Unstructured Sparsity</h3> <p>A significant challenge with many DST methods like RigL is that they typically produce <strong>unstructured sparsity</strong>. This means individual weights are zeroed out irregularly across the weight matrices.</p> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_unstructuredwmatrix.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_unstructuredwmatrix.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Untructured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Unstructured Sparsity. A neural network with unstructured sparse weights, and the corresponding weight matrix.</div> </div> <ul> <li><strong>Pros:</strong> Can achieve excellent generalization at very high sparsities (85-95%); fewer theoretical FLOPs.</li> <li><strong>Cons:</strong> Poorly supported by standard hardware (CPUs/GPUs) and acceleration libraries, meaning theoretical speedups often don’t translate into real-world gains.</li> </ul> <h3 id="structured-sparsity-eg-removing-neuronsblocks">Structured Sparsity (e.g., removing neurons/blocks)</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_structuredwmatrix.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_structuredwmatrix.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Structured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Structured Sparsity. A neural network with structured sparse weights, and the corresponding weight matrix.</div> </div> <p>In contrast, <strong>structured sparsity</strong> involves removing entire blocks of weights, such as channels, filters, or even neurons.</p> <ul> <li><strong>Pros:</strong> Much better hardware support, leading to practical speedups as it often results in effectively smaller dense operations.</li> <li><strong>Cons:</strong> Often leads to poorer generalization compared to unstructured sparsity at the same overall sparsity level, as it’s a coarser form of pruning.</li> </ul> <h3 id="nm-fine-grained-structured-sparsity">N:M Fine-grained Structured Sparsity</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_nmwmatrix.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_nmwmatrix.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="N:M Structured Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: N:M Structured Sparsity. A neural network with N:M structured sparse weights, and the corresponding weight matrix.</div> </div> <p>N:M fine-grained sparsity is a compromise where, within small contiguous blocks of M weights, exactly N weights are non-zero. NVIDIA’s Ampere GPUs support 2:4 sparsity, offering some acceleration <d-cite key="Mishra2021Accelerating,Nvidia2020Ampere"></d-cite>.</p> <p>The ideal scenario is to combine the high accuracy of unstructured DST with the hardware-friendliness of fine-grained structured sparsity.</p> <h2 id="dynamic-sparse-training-for-learning-structured-sparse-representations">Dynamic Sparse Training for Learning Structured Sparse Representations</h2> <p>Our work “Dynamic Sparse Training with Structured Sparsity” <d-cite key="Lasby2024SRigL"></d-cite> proposes a novel DST method called Structured RigL (SRigL) to address this challenge. SRigL aims to learn sparse networks that are both highly accurate <em>and</em> possess a structure amenable to real-world acceleration.</p> <h3 id="constant-fan-in-sparsity">Constant Fan-in Sparsity</h3> <p>SRigL modifies the RigL algorithm to enforce a <strong>constant fan-in</strong> constraint. This means each neuron (or output channel in a convolutional layer) has the same number of active incoming connections. This is a specific type of N:M sparsity (where N is the fan-in and M is the potential dense fan-in) and results in a regular structure within the weight matrices. Theoretical analysis suggests that this constant fan-in constraint should not inherently impair training dynamics and might even offer slightly better output-norm variance compared to less constrained sparsity patterns, especially for very sparse networks.</p> <h3 id="the-hidden-trick-of-dst-neuron-ablation">The Hidden Trick of DST: Neuron Ablation</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_sparsetraining_neuronablation.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_sparsetraining_neuronablation.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Neuron Ablation." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Neuron Ablation in DST. When a neuron has too few weight to learn useful representations, DST methods learn to ablate, or remove, the entire neuron &mdash; effectively reducing the width of the layer.</div> </div> <p>A key empirical finding was that standard unstructured DST methods, like RigL, when pushed to very high sparsity levels (&gt;90%), implicitly learn to <strong>ablate neurons</strong> — that is, it they learn to remove neurons with very few weights, effectively reducing the width of layers.</p> <p>This neuron ablation appears crucial for maintaining generalization at extreme sparsities, but enforcing a naive constant fan-in constraint would prevent this, as it would force every neuron to maintain the same number of weights, even if those weights are not useful for learning.</p> <h3 id="the-srigl-algorithm">The SRigL Algorithm</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_constantfanin.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_constantfanin.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Constant Fan-in Fine-grained Sparsity." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: Constant Fan-in Fine-grained Sparsity. A form of N:M sparsity where every neuron on a layer is constained to have the same number of weights.</div> </div> <p>SRigL integrates the constant fan-in objective with an explicit neuron ablation mechanism. The core steps, adapted from RigL, are:</p> <ol> <li>Identify weights to prune (smallest magnitude) and potential connections to grow (largest gradient magnitude on zeroed weights).</li> <li>Count salient weights per neuron.</li> <li><strong>Ablate neurons</strong>: If a neuron has fewer salient weights than a defined threshold ($\gamma_{sal}$ multiplied by the target fan-in), it’s entirely pruned. Its designated weights are redistributed.</li> <li>Compute the new constant fan-in based on any ablated neurons.</li> <li>Prune the globally smallest magnitude weights.</li> <li>For each <em>active</em> neuron, regrow connections to meet the target constant fan-in, prioritizing those with the largest gradient magnitudes.</li> </ol> <p>This allows SRigL to learn both fine-grained constant fan-in sparsity <em>within</em> active neurons and coarser neuron-level structured sparsity.</p> <h2 id="key-results-performance-and-acceleration">Key Results: Performance and Acceleration</h2> <p>SRigL was evaluated on image classification tasks using CIFAR-10 (ResNet-18, Wide ResNet-22) and ImageNet (ResNet-50, MobileNet-V3, ViT-B/16) <d-cite key="Lasby2024SRigL"></d-cite>.</p> <h3 id="matching-dense-accuracy-with-structured-sparsity">Matching Dense Accuracy with Structured Sparsity</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_resnet18.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_resnet18.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-18/CIFAR-10" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_wide_resnet22.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_wide_resnet22.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Wide ResNet 22/CIFAR-10" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_resnet50.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_resnet50.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-50/ImageNet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_vit.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_vit.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ViT/ImageNet" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: (a) Percentage active neurons (i.e., not ablated) following RigL/SRigL training on ResNet-50/ImageNet (b) Sparse Fan-In vs. ViT layer index at the end of training with RigL at 90% sparsity.</div> </div> <p>SRigL with neuron ablation was shown to achieve generalization performance comparable to unstructured RigL and often close to the dense training baseline, even at high sparsities (e.g., 90-95%) across various architectures. Extended training further improved performance, similar to RigL.</p> <h3 id="the-importance-of-neuron-ablation">The Importance of Neuron Ablation</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_imagenet_perc_active.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_imagenet_perc_active.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-50/ImageNet." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-6 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_vit_rigl_fan_in.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_vit_rigl_fan_in.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Sparse Fan-in vs VIT layer index." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: (a) Percentage active neurons (i.e., not ablated) following RigL/SRigL training on ResNet-50/ImageNet (b) Sparse Fan-in vs. ViT layer index at the end of training with RigL at 90% sparsity.</div> </div> <p>The neuron ablation component was critical. Without it, SRigL’s performance lagged behind unstructured RigL at very high sparsities (&gt;90%) and with Vision Transformers. Enabling SRigL to ablate neurons restored performance to RigL levels. The percentage of active neurons (not ablated) learned by SRigL dynamically adapted with sparsity, mirroring RigL’s behavior. For Vision Transformers, SRigL’s performance was particularly sensitive to the ablation threshold $\gamma_{sal}$, with higher thresholds performing best, suggesting that aggressively ablating neurons to maintain sufficient density in the remaining ones is beneficial for ViTs.</p> <h3 id="real-world-speedups">Real-World Speedups</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-md-12 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_grouped-bar-threads-4.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_grouped-bar-threads-4.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="ResNet-50/ImageNet." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-md-12 mt-3 mt-md-0 bg-white"> <figure> <picture> <source class="responsive-img-srcset" srcset="/spectrum-website/assets/img/srigl_grouped-bar-threads-4-device-gpu-batch_size-2048-broken-y.svg" sizes="95vw"/> <img src="/spectrum-website/assets/img/srigl_grouped-bar-threads-4-device-gpu-batch_size-2048-broken-y.svg" class="img-fluid rounded z-depth-0" width="100%" height="auto" title="Sparse Fan-in vs VIT layer index." loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption">Figure: (a) Percentage active neurons (i.e., not ablated) following RigL/SRigL training on ResNet-50/ImageNet (b) Batched GPU inference with batch size of 2048 on an NVIDIA Titan V. At 90% sparsity, our condensed representation is 1.7$\times$ faster than dense and 13.0$\times$ faster than unstructured (CSR) sparse layers. Note y-axis is log-scaled..</div> </div> <p>The structured sparsity learned by SRigL (constant fan-in + ablated neurons) translates into tangible inference speedups. The paper demonstrates a “condensed” matrix multiplication method (Algorithm 1 in the paper ) that leverages this structure.</p> <ul> <li><strong>CPU (Online Inference, single input):</strong> At 90% sparsity, SRigL’s condensed representation was up to <strong>3.4x faster than dense</strong> and 2.5x faster than unstructured (CSR) sparse layers on an Intel Xeon CPU.</li> <li><strong>GPU (Batched Inference, batch size 256):</strong> At 90% sparsity, it was <strong>1.7x faster than dense</strong> and 13.0x faster than unstructured (CSR) sparse layers on an NVIDIA Titan V GPU.</li> </ul> <p>These speedups are achieved even with a straightforward PyTorch implementation, highlighting the practical benefits of the learned structure.</p> <h3 id="not-only-efficiency-srigl-enables-new-applications-of-neural-networks">Not Only Efficiency: SRigL Enables New Applications of Neural Networks</h3> <p>SRigL’s structured sparsity is not just about speed; it also opens up new avenues for neural networks. The ability to learn a combination of fine-grained constant fan-in and neuron-level structured sparsity enables otherwise infeasible applications with neural networks.</p> <p>One interesting use case is in <strong>extreme classification</strong>, where the number of classes can reach millions. Representing such a large number of classes with dense models is impractical due to the sheer size and complexity of the model. For instance, in a typical image classification task with 1 million classes, a dense model would require a weight matrix of size $1 \text{M} \times 1 \text{M}$, which is not only computationally expensive but also memory-intensive. Already, SRigL has been successfully applied to extreme classification in our follow-up NeurIPS 2024 work <a href="https://openreview.net/forum?id=RA6rzOJ2zI">“Navigating Extremes: Dynamic Sparsity in Large Output Spaces”</a> in collaboration with Aalto University and the University of Bath <d-cite key="Ullah2024NavigatingExtremes"></d-cite>.</p> <p>The same problem applies to other tasks like natural language processing (NLP) and recommendation systems, where the number of classes can be extremely large, and SRigL’s ability to learn structured sparsity can help in efficiently representing and processing these large output spaces.</p> <h2 id="conclusion-and-future-horizons">Conclusion and Future Horizons</h2> <p>“Dynamic Sparse Training with Structured Sparsity” <d-cite key="Lasby2024SRigL"></d-cite> makes a significant stride towards practical sparse neural networks. SRigL demonstrates that it’s possible to:</p> <ul> <li>Train networks from a sparse initialization to a sparse solution (sparse-to-sparse).</li> <li>Achieve generalization performance on par with state-of-the-art <em>unstructured</em> sparse training methods.</li> <li>Learn a combination of fine-grained constant fan-in and neuron-level structured sparsity.</li> <li>Realize significant real-world inference acceleration on both CPUs and GPUs due to this learned structure.</li> </ul> <p>The insight that successful DST methods at high sparsity inherently learn to reduce model width (neuron ablation) is key and SRigL formalizes this. This work underscores that much of the progress in deep learning comes from methods that better leverage hardware capabilities.</p> <ul> <li>Improving the convergence speed of DST methods, which can take longer to train than dense models.</li> <li>Exploring the potential of DST to learn novel, efficient architectures for new data domains beyond typical NLP/CV tasks, particularly in areas like “AI for Science.”</li> </ul> <p>SRigL paves the way for deploying highly efficient and accurate sparse models in a wider range of applications, making powerful AI more accessible and sustainable.</p> <h2 id="citing-our-work">Citing our work</h2> <p>If you find this work useful, please consider citing it using the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lasby2024srigl</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lasby, Mike and Golubeva, Anna and Evci, Utku and Nica, Mihai and Ioannou, Yani}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">venue</span> <span class="p">=</span> <span class="s">{Vienna, Austria}</span><span class="p">,</span>
  <span class="na">eventdate</span> <span class="p">=</span> <span class="s">{2024-05-07/2024-05-11}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Sparse Training with Structured Sparsity}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">arxivid</span> <span class="p">=</span> <span class="s">{2305.02299}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2305.02299}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span>
<span class="p">}</span>
</code></pre></div></div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Dynamic Sparse Training with Structured Sparsity&quot; (<a href="https://t.co/MCxVCeMYt0">https://t.co/MCxVCeMYt0</a>) was accepted at ICLR 2024! DST methods learn state-of-the-art sparse masks, but accelerating DNNs with unstructured masks is difficult. SRigL learns structured masks, improving real-world CPU/GPU timings <a href="https://t.co/zZASJlXtRi">pic.twitter.com/zZASJlXtRi</a></p>&mdash; Mike Lasby (@mikelasby) <a href="https://twitter.com/mikelasby/status/1749622255339094128?ref_src=twsrc%5Etfw">January 23, 2024</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div>]]></content><author><name>Mike Lasby</name></author><category term="dynamic"/><category term="sparse"/><category term="training"/><category term="structured"/><category term="sparsity"/><category term="compression"/><category term="RigL"/><category term="SET"/><summary type="html"><![CDATA[Learning Performant and Efficient Representations suitable for Hardware Acceleration]]></summary></entry><entry><title type="html">Gradient Flow in Sparse Neural Networks &amp;amp; Why Lottery Tickets Win</title><link href="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2022/gradient-flow-sparse-neural-networks/" rel="alternate" type="text/html" title="Gradient Flow in Sparse Neural Networks &amp;amp; Why Lottery Tickets Win"/><published>2022-02-24T00:00:00+00:00</published><updated>2022-02-24T00:00:00+00:00</updated><id>https://spectrum-lab-iisc.github.io/spectrum-website/blog/2022/gradient-flow-sparse-neural-networks</id><content type="html" xml:base="https://spectrum-lab-iisc.github.io/spectrum-website/blog/2022/gradient-flow-sparse-neural-networks/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>Training sparse neural networks directly from a random initialization is notoriously difficult, often resulting in poor performance compared to their dense counterparts. The paper “Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win” <d-cite key="Evci2022GradientFlow"></d-cite>, accepted for an <a href="https://aaai-2022.virtualchair.net/poster_aaai3082">Oral Presentation at AAAI 2022</a>, investigates this through the perspective of gradient flow and finds:</p> <ul> <li><strong>Poor Gradient Flow at Initialization:</strong> Standard initialization techniques, designed for dense networks, are ill-suited for sparse networks due to their heterogeneous connectivity. This leads to vanishing gradients right from the start. We propose a sparsity-aware initialization that can alleviate this.</li> <li><strong>Poor Gradient Flow During Training:</strong> Even if initialized better, sparse networks can suffer from weak gradient flow throughout training. Dynamic Sparse Training (DST) methods, which adapt network connectivity during training, can significantly improve this.</li> <li><strong>Lottery Tickets Re-learn, Don’t Magically Fix Flow:</strong> The success of Lottery Tickets (LTs) isn’t due to them inherently having better gradient flow. Instead, LTs (which use specific initial weights from a pre-trained dense model’s history) effectively “re-learn” the good solution found by pruning the original dense model. They are guided to a known good basin of attraction, rather than finding a new one through superior optimization dynamics in a sparse setting.</li> </ul> <h2 id="the-quest-for-efficient-yet-powerful-neural-networks">The Quest for Efficient Yet Powerful Neural Networks</h2> <p>Deep Neural Networks (DNNs) are the powerhouses behind many AI breakthroughs. However, their increasing size and computational appetite pose significant challenges for deployment and training sustainability. One promising avenue for efficiency is <strong>sparsity</strong>: using networks with far fewer connections (and thus parameters) than typical dense networks.</p> <p>A common way to obtain a sparse network is by <strong>pruning</strong> a large, trained dense network. This often yields sparse models that retain the performance of the original dense model with a fraction of the parameters.</p> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/sparsetrainingproblem.svg" alt="Training Outcomes: Pruning pipeline vs. sparse training problem." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 1: (Left) The standard pruning pipeline: train a dense model, prune it, and optionally fine-tune to get a good sparse model. (Right) The sparse training problem: initializing a sparse network randomly and training it often leads to poor performance compared to the pruned model.</div> </div> <p>However, what if we want to train a sparse network from the get-go, without the costly pre-training of a dense model? This is where things get tricky. Naively initializing a network with a sparse structure and training it from scratch (the “sparse training problem”) usually leads to significantly worse performance. This begs the question: why is training sparse networks so hard, and what can we learn from the exceptions?</p> <h2 id="the-importance-of-gradient-flow">The Importance of Gradient Flow</h2> <p>Many advancements in training dense DNNs have come from understanding and improving <strong>gradient flow</strong> – how the error signals propagate backward through the network to update the weights. Poor gradient flow can lead to vanishing or exploding gradients, making training stall or become unstable. This paper <d-cite key="Evci2022GradientFlow"></d-cite> applies this lens to sparse neural networks.</p> <h2 id="problem-1-off-to-a-bad-start--poor-gradient-flow-at-initialization">Problem 1: Off to a Bad Start — Poor Gradient Flow at Initialization</h2> <h3 id="dense-initialization-and-sparse-fan-in">Dense Initialization and Sparse Fan-in</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/dense_fanin.svg" alt="Dense fan-in in a neural network." class="img-fluid rounded z-depth-0" loading="eager"/> <div class="caption">(a) Dense layer, every neuron has the same number of incoming connections</div> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/sparse_fanin.svg" alt="Heterogeneous sparse fan-in in a neural network." class="img-fluid rounded z-depth-0" loading="eager"/> <div class="caption">(b) Sparse layer, every neuron can have a different number of incoming connections</div> </div> </div> <div class="caption">Figure: (a) In a dense layer, each neuron has the same fan-in, (b) However, in a general unstructured sparse layer, the fan-in can vary significantly from neuron to neuron &mdash; we propose an initialization that accounts for this.</div> </div> <p>Standard weight initialization methods like Glorot/He <d-cite key="Glorot2010Understanding,He2015Delving"></d-cite> are designed with dense networks in mind. They assume that all neurons in a layer have the same number of incoming (fan-in) and outgoing (fan-out) connections.</p> <p>For example, for a layer using a ReLU activation function, the weights are initialized from a distribution with variance inversely proportional to the largest number of incoming connections (fan-in):</p> \[w_{ij}^{[l]} \sim \mathcal{N}(0, \frac{2}{\text{fan-in}}),\] <p>where $\textbf{fan-in}$ is the number of incoming connections for the layer, and $w_{ij}^{[l]}$ is the weight connecting neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$. This ensures that the variance of the output of the layer is roughly equal to the variance of its input, which helps maintain a healthy signal flow through the network.</p> <h3 id="a-sparsity-aware-initialization-to-fix-gradient-flow-at-initialization">A Sparsity-Aware Initialization to Fix Gradient Flow at Initialization</h3> <div class="container"> <div class="row align-items-center justify-content-center"> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/sparse_fanin_unequalout.svg" alt="Sparse neural network with dense init." class="img-fluid rounded z-depth-0" loading="eager"/> <div class="caption">(a) Dense initialization assumes every neuron has same number of connections, and on average, uses weights that are too small</div> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/sparse_fanin_equalout.svg" alt="Sparse neural network with sparse init.." class="img-fluid rounded z-depth-0" loading="eager"/> <div class="caption">(b) Sparse initialization calculates the correct weight variance for each neuron based on the number of incoming connections</div> </div> </div> <div class="caption">Figure: (a) Using a dense initialization for a sparse layer causes vanishing gradients as neurons with few connections are initialized incorrectly, however in (b) the sparse initialization accounts for the fact that fan-in can vary significantly from neuron to neuron &mdash; ensuring better behaved gradients at initialization.</div> </div> <p>In a sparse neural network, the assumption that every neuron has the same number of connections breaks down. Rather, the number of connections per neuron can be highly variable. Using dense initializations directly in sparse networks often causes the signal to vanish rapidly as it propagates through the layers. This is because neurons with fewer incoming connections are initialized with weights that are too small, leading to a weak signal. This can cause the gradients to vanish, especially in deeper networks, making it hard for the model to learn effectively.</p> <p>Our work <d-cite key="Evci2022GradientFlow"></d-cite> proposes a <strong>sparsity-aware initialization</strong> that adjusts the variance of the initial weights for each neuron based on its <em>actual</em> fan-in within the sparse structure, for example for a layer with a ReLU activation function:</p> \[w_{ij}^{[l]} \sim \mathcal{N}(0, \frac{2}{\text{fan-in}_i^{[l]}}),\] <p>where $\text{fan-in}_i^{[l]}$ is the number of incoming connections for neuron $i$ in layer $l$.</p> <h3 id="signal-propagation-at-initialization-with-different-initializations">Signal Propagation at Initialization with Different Initializations</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-8 mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/sparseinit_signalprop.svg" alt="Signal Propagation at Initialization: Graph showing standard deviation of output vs. sparsity." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 3: Standard deviation of the pre-softmax output ($\sigma(f(x))$) in LeNet-5 vs. sparsity level. Dense initialization (blue) shows signal vanishing with increasing sparsity. Sparsity-aware initializations (Liu et al. <d-cite key="Liu2019Rethinking"></d-cite> and "Ours" - the paper's proposal) maintain signal strength.</div> </div> <p>This sparsity-aware initialization leads to better signal propagation at the start of training and can improve the final generalization performance, especially for networks without normalization layers like BatchNorm (e.g., LeNet5, VGG16). For models with BatchNorm (e.g., ResNet-50), the effect of initialization is less pronounced, as BatchNorm itself helps regulate signal propagation.</p> <h2 id="problem-2-slogging-through--poor-gradient-flow-during-training">Problem 2: Slogging Through — Poor Gradient Flow During Training</h2> <p>While a good initialization helps, it’s not the whole story. Sparse networks can still suffer from poor gradient flow <em>during</em> the training process.</p> <div class="container"> <div class="row justify-content-center align-items-center bg-white"> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/mnist_gradnorm_logx.svg" alt="Gradient Norm During Training: Graphs for LeNet-5, VGG-16, and ResNet-50." class="img-fluid rounded z-depth-0" loading="eager"/> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/vgg_gradnorm.svg" alt="Gradient Norm During Training: Graphs for LeNet-5, VGG-16, and ResNet-50." class="img-fluid rounded z-depth-0" loading="eager"/> </div> <div class="col-6 mt-3 mt-md-0"> <img src="/spectrum-website/assets/img/gradientflow/resnet_gradnorm.svg" alt="Gradient Norm During Training: Graphs for LeNet-5, VGG-16, and ResNet-50." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 4: Gradient norm during training for LeNet-5 (left), VGG-16 (center), and ResNet-50 (right) under different setups. 'Scratch' (training a sparse mask from random dense initialization) often shows very low gradient norm initially. 'Scratch+' (with sparsity-aware initialization) improves this. 'RigL+' (a DST method with sparsity-aware init) often shows stronger gradient flow.</div> </div> <p>This is where <strong>Dynamic Sparse Training (DST)</strong> methods come in. DST techniques, like RigL <d-cite key="Evci2020RigL"></d-cite>, don’t keep the sparse connectivity fixed. Instead, they periodically update the mask during training:</p> <ol> <li><strong>Prune:</strong> Remove connections that have become less salient (e.g., small magnitude weights).</li> <li><strong>Grow:</strong> Add new connections, often by identifying those that would have the largest gradient if they were active.</li> </ol> <p>The paper shows that DST methods, particularly RigL, significantly improve gradient flow during training compared to training with a fixed sparse mask. These updates can introduce new directions for optimization (e.g., by creating new negative eigenvalues in the Hessian), helping the network escape poor regions of the loss landscape. This improved gradient flow correlates with better generalization performance.</p> <h2 id="the-curious-case-of-lottery-tickets-lts">The Curious Case of Lottery Tickets (LTs)</h2> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/lotterticketsolution.svg" alt="Lottery Ticket Hypothesis Concept: Diagram illustrating the LTH process." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 5: The Lottery Ticket Hypothesis: A dense network is trained (obtaining a dense solution), then pruned. The "winning ticket" uses the *initial weights* ($\Theta_{t=0}$ or an early snapshot $\Theta_{0&lt;t \ll T}$) corresponding to the pruned mask and is then trained.</div> </div> <p>The Lottery Ticket Hypothesis (LTH) <d-cite key="Frankle2019LTH"></d-cite> proposed that within a large, randomly initialized dense network, there exist smaller subnetworks (the “winning tickets”). If these winning tickets are trained in isolation from their <em>original initialization weights</em> (or weights from very early in the dense model’s training, known as “late rewinding”), they can achieve accuracy comparable to the full dense network.</p> <p>This was exciting because it suggested a way to find highly sparse, trainable networks. However, the paper <d-cite key="Evci2022GradientFlow"></d-cite> finds something intriguing: <strong>Lottery Tickets also exhibit poor gradient flow, similar to naively trained sparse networks!</strong> (see Figure 4).</p> <p>So, if LTs don’t fix the gradient flow problem, why do they work so well? The paper’s central argument is that LTs succeed because they essentially <strong>re-learn the pruning solution</strong> they were derived from.</p> <h3 id="evidence-for-lts-re-learning-the-pruning-solution">Evidence for LTs Re-learning the Pruning Solution</h3> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-9 mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/mnist_mds.svg" alt="MDS Plot of Solutions: 2D projection of solution distances for LeNet5." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 6: A 2D MDS projection showing the relative distances between different solutions for LeNet5. 'Lottery-start' is closer to 'Prune-end' than 'Scratch-start'. 'Lottery-end' converges very close to 'Prune-end', while 'Scratch-end' solutions are more dispersed and further away.</div> </div> <ol> <li><strong>Proximity in Weight Space:</strong> LT initializations (the specific weight values rewound from early in dense training) start much closer in L2 distance to the final <em>pruned solution</em> (the weights of the dense model after pruning) than a random “scratch” initialization using the same mask. After training, the LT solution ends up significantly closer to this pruned solution.</li> </ol> <div class="container"> <div class="row justify-content-center align-items-center"> <div class="col-6 mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/mnist_interpol.svg" alt="Loss Interpolation: Graph showing training loss along interpolation paths for LeNet5." class="img-fluid rounded z-depth-0" loading="eager"/> </div> <div class="col-6 mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/resnet_interpol.svg" alt="Loss Interpolation: Graph showing training loss along interpolation paths for LeNet5." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 7: Training loss along a linear interpolation path between a starting point ($\alpha=0$, e.g., Lottery-start or Scratch-start) and the Pruned Solution ($\alpha=1$) for LeNet5. The path between 'Lottery End' and 'Pruned Solution' is relatively flat, indicating they are in the same basin. The path from 'Scratch End' often shows a barrier.</div> </div> <ol start="2"> <li><strong>Same Basin of Attraction:</strong> By interpolating between the LT solution/initialization and the pruned solution, the paper shows that they lie within the same low-loss basin of attraction. In contrast, scratch solutions often have a high loss barrier separating them from the pruned solution’s basin.</li> </ol> <div class="container text-center align-items-center justify-content-center mx-auto"> <div class="caption"> Table 1: Ensemble &amp; Prediction Disagreement. We compare the function disagreement <d-cite key="Fort2019deepensembles"></d-cite> with the original pruning solution and ensemble generalization over 5 sparse ResNet50 models, trained from random initializations and LTs (lottery tickets) on ImageNet. As a baseline, we also show results for 5 pruned models trained from different random initializations. * we compare 4 different pruned models with the pruning solution LT are derived from. </div> <table> <thead> <tr> <th style="text-align: left"><strong>Initialization</strong></th> <th><strong>(Top-1) Test Accuracy</strong></th> <th><strong>Ensemble</strong></th> <th><strong>Disagreement</strong></th> <th><strong>Disagreement w/ Pruned</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left">LT</td> <td>75.73 ± 0.08</td> <td>76.27</td> <td>0.0894 ± 0.0009</td> <td>0.0941 ± 0.0009</td> </tr> <tr> <td style="text-align: left">Scratch</td> <td>71.16 ± 0.13</td> <td>74.05</td> <td>0.2039 ± 0.0013</td> <td>0.2033 ± 0.0012</td> </tr> <tr> <td style="text-align: left">Pruned Soln.</td> <td>75.60</td> <td>–</td> <td>–</td> <td>–</td> </tr> <tr> <td style="text-align: left">5 Diff. Pruned</td> <td>75.65 ± 0.13</td> <td>77.80</td> <td>0.1620 ± 0.0008</td> <td>0.1623 ± 0.0011*</td> </tr> </tbody> </table> </div> <ol start="3"> <li><strong>Functional Similarity:</strong> LT solutions are not only close in weight space but also learn very similar functions to the pruned solution they originated from. This is measured by low “disagreement” (fraction of test images classified differently) between the LT solution and the pruned solution. Ensembles of LTs derived from the same pruning process show little performance gain, further suggesting they converge to nearly identical functions.</li> </ol> <h3 id="the-lottery-ticket-initialization-a-nudge-in-the-right-direction">The Lottery Ticket Initialization: A Nudge in the Right Direction</h3> <div class="container l-screen"> <div class="row justify-content-center align-items-center"> <div class="col-lg mt-3 mt-md-0 bg-white"> <img src="/spectrum-website/assets/img/gradientflow/initializations_explained.svg" alt="Loss Landscape Intuition: Diagram illustrating basins of attraction for LT and Scratch." class="img-fluid rounded z-depth-0" loading="eager"/> </div> </div> <div class="caption">Figure 8: An intuitive illustration. A Lottery Ticket initialization (blue circle) is already positioned within the basin of attraction of the good Pruning Solution (green circle). Random (Scratch) initializations (red circles) are more likely to fall into different, potentially suboptimal, basins.</div> </div> <p><strong>The implication is powerful:</strong> LTs aren’t discovering new, highly effective sparse configurations through superior optimization dynamics. Instead, their specific initialization “nudges” the optimization process to rediscover a known good solution – the one found by pruning the dense network.</p> <h2 id="key-insights-summarized">Key Insights Summarized</h2> <p>This investigation into gradient flow in sparse neural networks reveals:</p> <ol> <li><strong>Sparsity-Aware Initialization Matters:</strong> Naive use of dense initializations harms sparse networks by causing poor gradient flow from the start. Using initializations that account for the actual sparse connectivity is crucial.</li> <li><strong>Dynamic Sparse Training Boosts Gradient Flow:</strong> DST methods improve gradient flow <em>during</em> training by adapting the network’s sparse connections, leading to better generalization than training with fixed sparse masks.</li> <li><strong>Lottery Tickets are “Echoes” of Pruning:</strong> LTs work well not because they inherently possess better gradient flow, but because their specific initial weights guide them to re-learn the solution of the pruned dense model they originated from. This limits their ability to find truly novel solutions in the sparse regime.</li> </ol> <h2 id="conclusion-and-future-directions">Conclusion and Future Directions</h2> <p>Understanding gradient flow provides valuable insights into the challenges of training sparse neural networks. While sparsity-aware initializations and Dynamic Sparse Training offer promising avenues for improving how we train sparse models from scratch, the success of Lottery Tickets seems more about “remembering” a good solution than fundamentally solving the optimization difficulties in sparse landscapes.</p> <p>The journey towards efficiently training sparse neural networks that are as performant as their dense counterparts, without relying on dense pre-training or specific “winning ticket” initializations, continues. Methods that can robustly navigate the complex loss landscapes of sparse models and maintain healthy gradient flow are key to unlocking the full potential of sparse training of neural networks.</p> <h2 id="citing-our-work">Citing our work</h2> <p>If you find this work useful, please consider citing it using the following BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">evci2022gradientflowsparse</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Evci, Utku and Ioannou, Yani A. and Keskin, Cem and Dauphin, Yann}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 36th AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">venue</span> <span class="p">=</span> <span class="s">{Vancouver, BC, Canada}</span><span class="p">,</span>
  <span class="na">eventdate</span> <span class="p">=</span> <span class="s">{2022-02-22/2022-03-1}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">arxivid</span> <span class="p">=</span> <span class="s">{2010.03533}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2010.03533}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1609/aaai.v36i6.20611}</span>
<span class="p">}</span>
</code></pre></div></div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Join us for our <a href="https://twitter.com/hashtag/AAAI2022?src=hash&amp;ref_src=twsrc%5Etfw">#AAAI2022</a> *Oral Presentation* of our paper on Sparse DNN training: &quot;Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win&quot; tomorrow (Feb 24) @ 10:30-11:45am PST!<br/>Poster at 8:45-10:30am/Feb 27 4:45-6:30pm. Work with <a href="https://twitter.com/utkuevci?ref_src=twsrc%5Etfw">@utkuevci</a>, <a href="https://twitter.com/ynd?ref_src=twsrc%5Etfw">@ynd</a>, <a href="https://twitter.com/cem_keskin_?ref_src=twsrc%5Etfw">@cem_keskin_</a>. <a href="https://t.co/BZLiRqDDzZ">pic.twitter.com/BZLiRqDDzZ</a></p>&mdash; Yani Ioannou (@yanii) <a href="https://twitter.com/yanii/status/1496532190682927107?ref_src=twsrc%5Etfw">February 23, 2022</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div>]]></content><author><name>Utku Evci</name></author><summary type="html"><![CDATA[An exploration of why sparse neural networks are hard to train and how understanding gradient flow sheds light on Lottery Tickets and Dynamic Sparse Training.]]></summary></entry></feed>